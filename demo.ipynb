{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get a warning that some model weights are not needed. This comes from `bert-base-uncased` having layers for other tasks (specifically, classification tasks) that our Masked Language Modeling task won't need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu'\n",
    "MODEL_CHECKPOINT = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_CHECKPOINT).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `bert-base-uncased` the mask token is `'[MASK]'` with token id `103`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask token: [MASK]\n",
      "Mask token id: 103\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mask token: {tokenizer.mask_token}\")\n",
    "print(f\"Mask token id: {tokenizer.mask_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a test string with a mask token. We will\n",
    "1. Tokenize it\n",
    "1. Note the mask token's position in the tokenized sequence\n",
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"It was the best of [MASK], it was the worst of times\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([1, 15])\n",
      "Logits shape: torch.Size([1, 15, 30522])\n",
      "Reconstructed output: it was the best of times, it was the worst of times\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text sequence, put on GPU\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\").to(DEVICE)\n",
    "print(f\"Inputs shape: {inputs.input_ids.shape}\")\n",
    "\n",
    "# Get the mask token's index\n",
    "mask_index = torch.where(inputs.input_ids[0] == tokenizer.mask_token_id)[0].item()\n",
    "\n",
    "# Get the model's logits outputs\n",
    "logits = model(**inputs).logits\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "# Get the predicted token for the mask\n",
    "predicted_token_id = torch.argmax(logits[0, mask_index])\n",
    "\n",
    "# Replace mask token with predicted id\n",
    "inputs.input_ids[0, mask_index] = predicted_token_id\n",
    "output = tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "print(f\"Reconstructed output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "tensor(2335, device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "mask_index = torch.where(inputs.input_ids[0] == tokenizer.mask_token_id)[0].item()\n",
    "print(mask_index)\n",
    "\n",
    "print(torch.argmax(logits[0, mask_index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['times']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([2335])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted tokens: ['.', 'all', 'you', 'need', 'is', '.', '.']\n",
      "Predicted tokens: ['.', 'you', 'can', '.', 't', 'always', 'get', 'what', 'you', 'want', '.']\n"
     ]
    }
   ],
   "source": [
    "sequences = [\n",
    "    \"All you [MASK] is love\",\n",
    "    \"You can't [MASK] get what you want\"\n",
    "]\n",
    "\n",
    "for s in sequences:\n",
    "    inputs = tokenizer(s, return_tensors=\"pt\").to(DEVICE)\n",
    "    logits = model(**inputs).logits\n",
    "    predicted_token_ids = torch.argmax(logits, dim=-1)\n",
    "    predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
    "    print(f\"Predicted tokens: {predicted_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2017, 2064, 1005, 1056,  103, 2131, 2054, 2017, 2215,  102]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified code with a fill-mask pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\"fill-mask\", model=model,\n",
    "                     tokenizer=tokenizer, device=DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
